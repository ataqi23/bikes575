---
title: "Dev Template"
author: "Group 2"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
#==============================#
#       Import Libraries       #
#==============================#
library(tidyverse)
library(GGally)
library(cowplot)
library(nlme)
#==============================#
#        Global Settings       #
#==============================#
# Center ggplot titles
theme_update(plot.title = element_text(hjust = 0.5))
#=============================#
#        Obtain Dataset       #
#=============================#
# Source the wrangling scripts
source(file = "../R/wrangle.R")
# Import and wrangle data
bike_day <- read.csv("../data/bike-day.csv", header = T)
# Use the wrangling "wrapper" function to clean the data (fxn sourced from wrangle.R)
data <- wrangle_init(data = bike_day, omit_idx = FALSE)
# Partition the 2011/2012 data
data2011 <- in_2011(data) ; data2012 <- in_2012(data)
#============================#
#       Source Scripts       #
#============================#
# Import any models (and fitted values, if applicable)
source(file = "../R/model.R")
# Import any plots or plot wrapper functions
source(file = "../R/plot.R")
```


\newpage

## OLS Model: Paritioning the data by year

```{r}
ols.model1 <- function(data){
  lm(cnt ~ atemp + I(atemp^2) + holiday + weathersit, data)
}
```

The parameter estimates retain their signs if we split the dataset. From an inference POV, this is a good sign. Curiously enough, however, the actual parameter estimates change slightly.

## Training on 2011 Data

```{r}
ols.model1(data2011)
```

## Training on 2012 Data

```{r}
ols.model1(data2012)
```

\newpage

## Accounting for Growth in Bikeshare Users

There is a bit of a problem if we are to use the 2011 data as training data and 2012 as testing data. It assumes the number of users stays constant, which is certainly not true. \textbf{This is critical since our response variable is a count}. The number of bikesharing users has significantly grown since 2011 and continues to grow (could we cite something for this?) and treating the dataset as two disjoint datasets might be appopriate. Let's do a hypothesis test to see if $\mu_{2011} - \mu_{2012} = 0$.

```{r}
t.test(data2011$cnt, data2012$cnt)
```

## A possible solution

As such, we need to account for this, since in theory, the true parameter estimate $\hat{\beta}_{atemp}$ should be independent of how many users there are and inturn which year it is. One possible solution is to consider scaling down our response variables in the 2012 data by the following factor $r = \frac{\mu_{2011}}{\mu_{2012}}$. In other words, we want to fit the scaled count variables $\tilde{c_i} = rc_i$. Only this way can we truly and fairly assess the parameter fit $\hat{\beta}$. 

```{r}
# Get the ratio of means scaling factor
r <- mean(data2011$cnt)/mean(data2012$cnt)
# Create adjusted count variable in 2012
data2012_ADJ <- data2012 %>% mutate(cnt = r * cnt)
```

Now, let us fit both models and see how the parameter estimates hold up. The parameter estimates indeed seem much more stable!

```{r}
ols.model1(data2011)
ols.model1(data2012_ADJ)
```

Another way we could verify this is by using the adjusted count as a variable, reunifying the dataset, and fitting the same model.

```{r}
ols.adjusted <- lm(cnt_adj ~ atemp + I(atemp^2) + holiday + weathersit, data = data)
ols.adjusted
```

```{r}
residuals_adj <- data.frame(x = data$instant, y = ols.adjusted$residuals)
# Residual plot by instance
p1 <- ggplot(data = residuals_adj) + geom_point(aes(x, y))
```

```{r}
ols.unadjusted <- lm(cnt ~ yr + atemp + I(atemp^2) + holiday + weathersit, data = data)
ols.unadjusted
```

```{r}
residuals_unadj <- data.frame(x = data$instant, y = ols.unadjusted$residuals)
# Residual plot by instance
p2 <- ggplot(data = residuals_unadj) + geom_point(aes(x, y))
```

```{r}
hist(residuals_adj$y)
hist(residuals_unadj$y)
```


```{r}
plot_grid(p1, p2)
```

\newpage

# Model Performance

```{r}
# Train model on 2011 data (don't show it 2012 data)
ols.model <- ols.model1(data = data2011) 
#summary(ols.model)
```

```{r}
preds2012 <- predict(ols.model, data2012)
```

### MSE of unadjusted response on 2012 dataset (training on 2011 data only)

```{r}
# MSEs
MSE_unadj <- sqrt(mean((preds2012 - data2012$cnt)^2))
MSE_unadj
```

### MSE of adjusted response on 2012 dataset (training on 2011 data only)

```{r}
MSE_adj <- sqrt(mean((preds2012 - data2012$cnt_adj)^2))
MSE_adj
```

\textbf{In conclusion, adjusting the response variable shows that the model performs better than initially thought. By adjusting for an increasing base of bike share users, the parameter $\beta$ is "given a fair shot" to estimate the effects our predictors have since the adjusted response variable "relevels" the playing field.}

Now lets compare it to if we used the `yr` variable as a predictor, how does it compare? In theory, we have the same exact information, so we expect the model performance on the testing data to not be too disparate. Why would changing from a indicator variable scheme to a scaled response variable scheme change performance if both use the same "information"?

Surprisingly, the model with the scaled response variable scheme did perform better. So, if we account the response variable for expected growth $r$ rather than add a constant $\Delta \mu$ to our fitted values (the effect of regressing an indicator variable), this indicates we might get better model performance. 

```{r}
preds2012 <- predict(ols.unadjusted, data2012)
MSE <- sqrt(mean((preds2012 - data2012$cnt)^2))
MSE
```

\newpage

# Plot showing the scaled and unscaled data series

```{r}
rbind(data2011, data2012) %>%
  ggplot() +
  geom_point(aes(x = instant, y = cnt, color = atemp, shape = weathersit))
```

```{r}
data %>%
  ggplot() +
  geom_point(aes(x = instant, y = cnt_adj, color = atemp, shape = weathersit))
```






