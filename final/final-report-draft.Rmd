---
title: | 
  | Bike-Sharing Data Analysis: 
  | Prediction of Daily Bike Rental Counts 
  | Based on Multiple Linear Regression
subtitle: |
  | \hfill\break
  | Final Project Report · MA 575 Fall 2021 · C3 · Team #2
author: "Ali Taqi, Hsin-Chang Lin, Huiru Yang, Ryan Mahoney, Yulin Li"
date: "12/10/2021"
abstract: |
 | In this project, the following question is to be answered: If we have the past history of bike rental counts as well as records of environmental and seasonal conditions, how and how well could we predict the bike rental counts in the future? In this project, such questions are approached by predictive modeling of daily bike rental counts from a 2011-2012 Bike Sharing dataset [1]. The daily bike rental counts are predicted with models based on Multiple Linear Regression (MLS) using the environmental and seasonal variables as predictors. The initial goal of this project is to train the model using only the 2011 data, and then validate the prediction power of the model on the 2012 data. Given the limited time span of available training data, issues are found in the validation process using the 2012 data; the impact of user base on the future predictions is brought to our attention. The initial models are then revisited and corrected to account for the effect of user base. The refined models are expected to have better prediction powers than the initial MLS models, but a full validation would require further availability of bike rental data. 

output: 
  pdf_document: 
    number_sections: true
    includes:
      in_header: ["tex/preamble.tex", "tex/math.tex", "tex/tables.tex"]
classoption: twocolumn
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", warning = F, message = F, fig.height = 4, fig.width = 6)
#==============================#
#       Import Libraries       #
#==============================#
library(tidyverse)
library(GGally)
library(cowplot)
library(gglm)
library(moderndive)
library(patchwork)
#==============================#
#        Global Settings       #
#==============================#
# Center ggplot titles
theme_update(plot.title = element_text(hjust = 0.5))
#=============================#
#        Obtain Dataset       #
#=============================#
# Source the wrangling scripts
source(file = "wrangle.R")
# Import and wrangle data
bike_day <- read.csv("../data/bike-day.csv", header = T)
# Use the wrangling "wrapper" function to clean the data (fxn sourced from wrangle.R)
data <- wrangle_init(bike_day)
# Partition the 2011/2012 data
data2011 <- in_2011(data); data2012 <- in_2012(data)
# Add the weekly average columns
data2011 <- data2011 %>% add_weekly_averages
data2012 <- data2012 %>% add_weekly_averages
#============================#
#       Source Scripts       #
#============================#
# Import any models (and fitted values, if applicable)
source(file = "model-build.R")
source(file = "model.R")
# Import diagnostics functions
source(file = "diagnostics.R")
# Import any plots or plot wrapper functions
source(file = "plot.R")
# Source code for growth estimation
source(file = "growth.R")
```

# Introduction 

Bike sharing has become a world-wide phenomenon. Optimization of inventories and dynamic reallocation of bike-sharing resources are of growing interests from both a business and an environmental point of view. Both of these tasks require accurate predictions of bike rental behaviors at least on the daily level. 

In this project, we strive to answer the following question: 

- If we have the past history of bike rental counts as well as records of environmental and seasonal conditions, how and how well could we predict the bike rental counts in the future? 

- In particular, how and how well could we predict for the next whole year, and what about for the next few days? 

Such questions are approached by predictive modeling of daily bike rental counts from a 2011-2012 Bike Sharing dataset [1]. The modeling approach is based on Multiple Linear Regression (MLS), and the daily bike rental counts are predicted using the environmental variables (e.g., weather conditions) and seasonal variables (e.g., holiday schedules) as predictors.

# Background 

The aim of this project is to achieve the best model(s) that can be obtained from past data for the use of predictions for the future, preferably predictions one year ahead. To validate the prediction power of models under this setting, the basic goal of this project is to train all models using only the 2011 data, and then test them on the 2012 data. 

The response variable to be predicted is the **daily** bike rental count. In the dataset being studied [1], the following 3 types of bike rental counts are recorded: 

1. the count of bike rentals by **casual** users
2. the count of bike rentals by **registered** users
3. the **total** count, which is the sum of casual count and registered count. 

Two main types of predictors are included in the dataset, the environmental ones and the seasonal ones^[For a full explanation, see Appendix.]: 

1. **Environmental Variables^[Variable meanings (in order): weather type, measured temperature, feeling temperature, humidity, wind speed]:** `weathersit`, `temp`, `atemp`, `hum`, `windspeed`

2. **Seasonal Variables^[Variable meanings (in order): year, month, season, holiday, weekday]:** `yr`, `mnth`, `season`, `holiday`, `weekday`, `workingday`

# Modeling & Analysis 

## Pre-processing

### Type Conversion

To be noticed, the value of categorical variables indicates type labels and has very limited physical meaning in the magnitude of those values, which thus cannot be used in the same way as the numeric variables in MLS models. The categorical variables therefore needs to be recognized before the actual modeling process and to be carefully handled. 

The below variables are interpreted as Boolean variables and are transformed into `logical`-type variables in `R`:

- `holiday` (holiday or not)
- `workingday` (working day or not)

The below variables are interpreted as categorical variables and are transformed into `factor`-type variables in `R`:

- `season` (season, from 1 to 4)
- `yr` (year, from 0 to 1)
- `mnth` (month, from 1 to 12)
- `weekday` (weekday, from 0 to 6)
- `weathersit` (weather type, from 1 to 4)^[Note: larger index indicates worse weather]

### Value Conversion

The recorded values of `temp` (measured temperature), `atemp` (feeling temperature), `hum` (measured humidity) and `windspeed` (measured wind speed) in the data set being studied here are the normalized ones; all recorded values are the ones that have been divided by the maximum of measured values [1]. For example, the recorded values of `temp` (measured temperature) are obtained by dividing the original measured values by 41 (max) and are thus all less than or equal to 1.  

In this project, these normalized records are scaled back to their original values for the sake of easier interpretations. For example, the recorded values of `temp` (measured temperature) are multiplied by 41 (max) in the pre-processing process, which recovers the original scale of temperatures in Celsius. 

## Variable Selection 

### Response Transformation

Notably, the behaviors of rental counts from different user types are considerably different. 

1. **Patterns with weekdays** (see Figure 1^[Transparent plots of the casual counts in the background for easier comparison.], 2^[Transparent plots of the registered counts in the background for easier comparison.]): Over the time span of a week, the casual count usually reaches its minimum in the middle of a week (grey dots mostly) and its maximum on weekends (green dots mostly), while the registered count does the opposite. 

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- data2011 %>% plot_user_counts(user = "registered", compare = T)
  ggsave(filename = "graphics/reg-2011-simple.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Registered User Counts"}
knitr::include_graphics("graphics/reg-2011-simple.pdf")
```

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- data2011 %>% plot_user_counts(user = "casual", compare = T)
  myplot
  ggsave(filename = "graphics/cas-2011-simple.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Casual User Counts"}
knitr::include_graphics("graphics/cas-2011-simple.pdf")
```

2. **Patterns with temperatures** (see Figure 3): On **working** days, the casual count seems more linear in temperature (`atemp` and `temp`), while the registered count seems to be (at least) quadratic most of the time. 

```{r, fig.width= 8, height = 4, fig.cap = "Registered Counts v.s. Temperature (Left Subplot); Casual Counts v.s. Temperature (Right Subplot)"}
fig3 + fig4
```

We therefore expect that the registered counts and casual counts will follow different distributions and should thus be predicted by separate models. Furthermore, for the casual count, avoiding unnecessary higher other terms has the benefit of more stable computations and model structures. The prediction of total counts will then be obtained by adding the predicted registered counts and predicted casual counts together.  

### Predictor Selection

Given the predictive nature of modeling in the current problem setting, the predicted response is of greater interests than the actual value of the parameter estimates, as opposed to that in an inference task. This, to some degree, relaxes the constraint forbidding colinearity in the predictors, since colinearity will only lead to instability in the parameter estimates but not in the predictions; however, we should still seek to minimize colinearity at least in our beginning model, which would lead to clearer model structures as well as better interpretability of model statistics at the early stage of modeling, which could provide us clearer directions in the improvement process that follows. 

With the above considerations in mind, the predictors in the beginning model are selected following the 2-step approach below: 

1. The scatter plot matrix for the whole set of variables are plotted for the 2011 training dataset, and all predictors that seem to be significant, i.e., predictors with which the response variable (daily rental count, `cnt`) exhibits a notable visual pattern, are selected. 

2. From the selected predictors above, all the highly correlated predictors are removed. Within a group of correlated predictors, only the one that has the largest correlation coefficient with the response variable as well as having the strongest causal relation with the response (in the intuitive sense) will be kept. 

It is important that the investigation is done for all predictors for the sake of minimal loss of information. Note that in practice, the whole set of predictors is divided into two groups, environmental and seasonal, and plotted separately, for better readability of the large scatter plot matrices. The separation is justified by the fact that most environmental variables, such as weathers, are expected to be independent of the seasonal variables, such as weekdays and holiday schedules. 

At last, the above process leaves us with a small subset of the very core predictors for our beginning model: `weathersit`, `atemp` and `weekday`.

## Initial Modeling^[Models evaluated on: 1. RMSE (`rmse` in table); 2. normalized RMSE (RMSE divided by residual standard deviation) (`n-rmse` in table); 3. percentage error (`% Error` in table); 4. LOOCV RMSE (`cv-rmse` in table)]

In the model building and selection process, we start from the simplest models, which have the minimal number of predictors all in the additive form, as the beginning models.  

### Beginning Models

1. For **total count**: 
$$\text{cnt} \sim \text{wkngday + weathersit + atemp + atemp}^2$$
```{r echo=FALSE, include = F}
get_mod_eval_tot_2011(mod.tot.1.final)
```
\tableone
2. For **registered count**: 
$$\text{reg} \sim \text{wkngday + weathersit + atemp + atemp}^2$$ 
3. For **casual count**: 
$$\text{cas} \sim \text{wkngday + weathersit + atemp}$$


```{r echo=FALSE, include = F}
get_mod_eval_2011(mod.cas.1.final, mod.reg.1.final)
```

\tabletwo

The percentage error of 2011 predicted total counts computed by the sum of the registered and casual models is less than that computed by the single total model (see Table 1, 2), which demonstrates the power of separate modeling for registered and casual users. Therefore, we will continue with the scheme of separate modeling in the later part. 


### Procedures

Starting from the beginning models, we perform model building and selection in an iterative manner, for the registered and casual counts separately: 

- Starting from a (relatively) simpler version of the model, the 2011 fitted response using this model is plotted along with the actual response against time (see, for example, Figure 4, 5); the numeric metrics (RMSE, normalized RMSE, percentage error and LOOCV RMSE) are obtained as well (see, for example, Table 2). 

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2011reg_plot(mod.reg.1.final)
  ggsave(filename = "graphics/reg-2011-mod-1-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Fitted Registered Count v.s. Actual (Beginning Model)"}
knitr::include_graphics("graphics/reg-2011-mod-1-final.pdf")
```
```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2011cas_plot(mod.cas.1.final)
  ggsave(filename = "graphics/cas-2011-mod-1-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Fitted Casual Count v.s. Actual (Beginning Model)"}
knitr::include_graphics("graphics/cas-2011-mod-1-final.pdf")
```

- From the 2011 fitted versus actual plot, patterns in the biases can usually be visually recognized; combined with commonsense, this provides us with ideas of new variables potentially needed to account for the unexplained biases. 

For example, for the beginning models of registered and casual counts, both response variables are being consistently overestimated in the 2011 spring and underestimated in the later part of the year (see Figure 4, 5), which indicates that the users' response to weather and temperature might differ across seasons. This makes sense because each label of the weather type includes several different kinds of weathers (e.g., slightly snowy and slightly rainy days are both labeled as `3`), which could cast different level of difficulties on biking activities, and it is still insufficient to fully distinguish between those weathers given only temperature information. Additionally, the level of biases also differ considerably between workdays (grey dots) and weekends (mostly green dots), which indicates that the weekday variables might also be needed. 

- The new variables, one at a time, are then added to the simple model to create a more complex model. Both additive terms and interactive terms will be attempted. Then the version with the most significant improvement, according to the fitted versus actual plot and the numeric metrics, will be kept for the next round. 

In this iterative process of modeling, we look at the Leave-One-Out-Cross-Validation (LOOCV) RMSE as a proxy for the extent of overfitting. The model building process stops when the LOOCV RMSE starts to ramp up as model complexity increases, typically becoming considerably larger than the RMSE (compared to the previous models). 

Note that in this process, the model statistics (e.g., p-values) are also checked but are not relied on as much, out of the following two considerations: 

1. There is no guarantee in the normality of residual distributions as well as correct model forms, in which case the summary statistics might thus be invalid at all, especially in the intermediate stage of modeling with the incomplete models. 

2. As the model becomes more and more complex in this process, the colinearity issues worsen, which might weaken the significance of inter-correlated predictors (i.e., it might turn out that none of those predictors are indicated as significant in the summary table), while this is not to say that none of those predictors are necessary for the model to have more accurate predictions. 


### Final Models

At the end of the iterative modeling process, we arrive at the following final models^[For a more complete list of attempted models, see Appendix.]: 

\text{1.} For **registered count**: \newline
\chomp
\begin{equation*}
\begin{split}
  \text{reg} \sim \,\, &\text{holiday} + \text{season:weathersit} \,+  \\
    & \text{season:workingday:atemp}\, +  \\
    & \text{season:workingday:atemp}^2
\end{split}
\end{equation*}
\text{2.} For **casual count**: \newline
\chomp
\begin{equation*}
\begin{split}
  \text{cas} \sim \,\, &\text{holiday} + \text{season:weathersit}\, +  \\
    &\text{season:workingday:atemp}
\end{split}
\end{equation*}
```{r echo=FALSE, include = F}
get_mod_eval_2011(mod.cas.2.final, mod.reg.2.final)
```
\tablethree

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2011reg_plot(mod.reg.2.final)
  ggsave(filename = "graphics/reg-2011-mod-2-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Fitted Registered Count v.s. Actual (Final Model)"}
knitr::include_graphics("graphics/reg-2011-mod-2-final.pdf")
```

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2011cas_plot(mod.cas.2.final)
  ggsave(filename = "graphics/cas-2011-mod-2-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2011 Fitted Casual Count v.s. Actual (Final Model)"}
knitr::include_graphics("graphics/cas-2011-mod-2-final.pdf")
```

The final models are built with considerations of real-life experience and commonsense. For example, the interaction terms between feeling temperature, working day and seasons indicate people's different reaction to temperature change under different weathers on working days and on weekends. 

From this point on, adding any other variable to the model will result in a rise in the LOOCV RMSE, which indicates an overfitting issue that weakens the predictive power of the model. Notably, using the `weekday` variable in place of the `workingday` variable also worsens the performance. 

The final models achieve a ~10% improvement in the prediction percentage error in 2011 daily total counts as well as significant improvements in all the other metrics. The LOOCV RMSE is only ~15% higher than the training RMSE, indicating that the overfitting issue is still at a moderate level. Also, the systematic biases in the beginning models, as mentioned in the former section, have been alleviated (see Figure 6, 7). 

A diagnostic analysis (see Figure 8, 9) indicates that the constant-variance assumption and normality assumption of the registered model are mostly sound, while those of the casual model are more in doubt. Accordingly, the prediction errors of the casual model is also at a higher level than the registered model (see normalized RMSE and percentage error in Table 3). This situation can be understood intuitively, as the behavior of registered users is generally more regular and can be better captured by the simple MLS model, thus more predictable. Note that the least-square approach for the casual model is still valid, because the residuals are still uni-modal at the very least. 

```{r, fig.width = 10, fig.height = 3, fig.cap="Residual Diagnostic Plots for the Final Registered Model"}
mod.reg.2.final %>% residual_plots("")
```

```{r, fig.width = 10, fig.height = 3, fig.cap="Residual Diagnostic Plots for the Final Casual Model"}
mod.cas.2.final %>% residual_plots("")
```


## Prediction

### Initial Predictions

Despite the predictive power demonstrated by cross-validation metrics in the 2011 training set, the final models provide much worse predictions in 2012. 

```{r, echo=FALSE, include=FALSE}
evaluate_model(mod.cas.2.final, mod.reg.2.final)
```

\tablefour

```{r, echo=FALSE, include=FALSE}
evaluate_model(mod.cas.2.final, mod.reg.2.final, scale_2012 = T)
```

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2012reg_plot(mod.reg.2.final)
  ggsave(filename = "graphics/reg-2012-mod-2-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2012 Predicted Registered Count v.s. Actual (Final Model) (Initial Prediction)"}
knitr::include_graphics("graphics/reg-2012-mod-2-final.pdf")
```

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2012cas_plot(mod.cas.2.final)
  ggsave(filename = "graphics/cas-2012-mod-2-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2012 Predicted Casual Count v.s. Actual (Final Model) (Initial Prediction)"}
knitr::include_graphics("graphics/cas-2012-mod-2-final.pdf")
```

A closer look at the 2012 predictions reveals the fact that the daily rental counts are being consistently underestimated (see Figure 10, 11). 


### Adjusting Predictions for Growth

A comparison of the 2011 and 2012 rental counts reveals the reason for the systematic prediction error: the 2012 rental counts are generally higher than that in 2011. Furthermore, the 2012 predictions by the models trained in 2011 seem to be generally off by a constant factor. 

This observation leads to one possible explanation for the prediction error: 

Intuitively, the bike-rental behavior of an individual user may be modeled by a random variable, say, $X$. Then, the total bike rental count from all active users could then be modeled as a sum of independent identically distributed random variables $\sum_{i=1}^{n}{X_i}$, where $X_i \sim X$ and $n$ is the total number of active bike users. As a consequence, the expected value of total counts will be proportional to the user count, $n$, and so does the variance. 

Note that the effect of user base on the rental counts should be independent of all the other predictors. If the above hypothesis were true, then the correct models for 2012 bike rental counts should be the 2011 models scaled by the growth ratio of user counts.  

As such, an estimation of the growth ratio, which we call $\g$, is very important. By construction, its true value is well estimated by the ratio $\frac{\E_{2012}(C)}{\E_{2011}(C)} \approx (0.608)^{-1} = 1.64$, which is the average counts in 2012 divided those in 2011. Note that this means our user base grew by around $64\%$ between 2011-2012, which is a non-trivial amount. If unaccounted for, that magnitude of growth would (and does in fact) lead to chronic model underfitting. 

To verify the above hypothesis, we adjust the initial 2012 predictions by multiplying with $\g$. As a result, the adjusted predictions for both registered and casual counts now almost perfectly match the actual values (see Figure 12, 13). This provides a strong evidence for our accounts for the role of user base. 

\tablefive

To generalize for future predictions in practice, note that this modeling paradigm is based on the assumption that the growth trend within a year will remain the same in the future years as that in the year of 2011. However, this is NOT saying that the user base is supposed to remain unchanged throughout the entire year; the fact that the same scaling factor works at all points in the entire year is due to the fact that the MLS model in the later part of the year, e.g., in fall and winter, are already trained to compensate for rental count growth due to user growth using the environmental and seasonal variables. 

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2012reg_plot(mod.reg.2.final, scale_2012 = T)
  ggsave(filename = "graphics/reg-2012-mod-2-refined-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2012 Predicted Registered Count v.s. Actual (Final Model) (Adjusted Prediction)"}
knitr::include_graphics("graphics/reg-2012-mod-2-refined-final.pdf")
```

```{r, fig.height = 4.5, fig.width = 7.25}
update <- F
if(update)
{
  myplot <- predict2012cas_plot(mod.cas.2.final, scale_2012 = T)
  ggsave(filename = "graphics/cas-2012-mod-2-refined-final.pdf", plot = myplot)
}
```

```{r, out.width = "100%", fig.cap = "2012 Predicted Casual Count v.s. Actual (Final Model) (Adjusted Prediction)"}
knitr::include_graphics("graphics/cas-2012-mod-2-refined-final.pdf")
```

# The Growth Ratio

Adjusting the predictions with the growth ratio leads to significantly improved performance. However, since we are not \textbf{allowed} to peek at the 2012 data, we must resort to other creative techniques in estimating $\g$. In this report, we propose two techniques for estimating $\g$: the environmental loss function technique (more involved) and the window technique (more simple).

## Method I: Environmental Loss Function

The primary issue of relying solely on one years' worth of data to estimate the growth within that year is that there is great difficulty in factoring out the role environmental factors play in determining the difference in observed counts between any two given days. As such, if we are somehow able to find two days whose weather/environmental conditions are "similar" or "equivalent", the ratio of the counts between the later day and the earlier one is a reasonable "data point" useful for obtaining a sense of the growth. This follows because we asked those two days to be "environmentally equivalent" (we will formalize this notion soon), so \textit{a priori}, if we were somehow able to marginalize out the environmental factors, then any observed difference in the count `cnt` \textbf{must} be attributed to user base growth! This motivates our technique, aptly named the "environmental loss function" technique. In brief, here is the strategy: by designing a loss function, we seek to find special pairs of days whose environmental factors are roughly equivalent. By taking the ratio of the counts, we obtain a preliminary estimate for the growth factor since in theory, we have marginalized the environmental factors in the selection process. Aggregating all these ratios and averaging them, we obtain an estimate for $\g$. That being said, we now slowly develop our technique. 

Suppose we have two days (observations) $d$ and $d'$. Consider the reduced dataset where we only have the three predictors: `atemp`, `hum`, and `windspeed`. In other words, our observations are vectors solely comprised of our \textit{continous environmental variables}. Furthermore, suppose that we \textbf{standardize} these variables. This is critical since we don't want our loss functions' units to be arbitrarily inflated/deflated, but rather, unit-agnostic. This is one reason we might prefer continuous variables when implementing our loss function. Also, let's use `holiday` to focus only on non-holidays and removing holiday days from the dataset. We can adjust for environmental factors but comparing two "environmentally equivalent" days without accounting for `holiday` may very well throw off our estimates of $\hat{g}$. Losing the 11 days which are holidays prove to be trivial, since we are counting pairs and $\binom{365}{2} \approx \binom{354}{2}$. Note that we omit `weathersit` and `workingday` (albeit the latter is not environmental, it is still important) mainly since they are categorical variables. While loss functions can include categorical variables, we will opt for the simplicity, familiarity, and stability of continous variables. In any case, we can justify their omission anyway since `weathersit` is correlated to our chosen environmental variables, so the information loss is not catastrophic. Additionally, since we are using `cnt` and not the subdivisions of `cas` and `reg`, the explanatory power of `workingday` is "canceled" out in the `cnt` variable in a sense since we observe inverse behavior of `cas` and `reg` with respect to `workingday`. Now, we may put the discussion of chosen predictors aside.

That being said, this means we may write $d = (x_1, x_2, x_3)$ and $d' = (x_1', x_2', x_3')$ where $x_i$ represents the \textbf{standardized} value of predictor $i$. Furthermore, let $\vec{\alpha} = (\alpha_1, \alpha_2, \alpha_3)$ be a weight vector, more heavily weighing variables deemed more important in determining "environmental similarity". We deem this to be `atemp`. In practice, we end up weighing `atemp` by $\alpha_1 = \frac{2}{3}$ and the other two predictors, `hum` and `windspeed`, by $\alpha_2 = \alpha_3 = \frac{1}{6}$. Note that this is arbitrary, and subject to scrutiny, but for the sake of simplicity, we may all agree temperature is the major determinant in constituting a notion of "environmental equivalence". 

Finally, let $\delta_i$ denote the weighted difference between in the values of predictor $i$ between day $d$ and $d'$. So, we write $\delta_i = \alpha_i(x_i - x_i')$. Observing the differences among all our predictors, we obtain the difference vector $\vec{\delta} = (\delta_i)_{i=1}^3$. Finally, a very natural notion of "loss function" arises: the magnitude of the difference vector! We restate this as such. Consider the loss function $\L: \R^3 \times \R^3 \to \R^+$, which takes two days as input, and outputs the magnitude of weighted vector of the differences in our normalized predictor variables between them. We write the \textit{environmental equivalence loss} of $d'$ with respect to $d$ as follows:
$$\mathcal{L}_d(d') = |\vec{\delta}| \text{ where } \delta_i = \alpha_i(x_i - x_i')$$
Now, suppose we enumerate \textbf{all possible unique ordered pairs} of days, which we denote $\Pi$: 
$$\Pi = \{\pi_{ij} = (d_i, d_j) \mid i < j\}$$ 
Note that we use the lower-triangle scheme, where we choose $i < j$ so that $i - j < 0$. This will prove useful later on since we want day $i$ to preceed day $j$. We call this the "lower-triangular" scheme because this corresponds to the unique pairs of indices found in the "lower triangle part" of matrix. In any case, we compute the loss for every pair, and call it $l_{ij} = \mathcal{L}(\pi_{ij})$ . Then, we obtain the following set: 
$$\mathcal{L}(\Pi) = \{l_{ij} = \mathcal{L}(\pi_{ij}) \mid i < j\}$$
This is an exhaustive and computationally expensive procedure, since we compute the loss for $\binom{354}{2} \approx 62,000$ pairs of days! So, in practice we compute this once and store the data in `df_loss.csv`. When imported, we conventionally call the dataframe `df_loss`. 

Now, here is the magic! We are not interested in taking pairs $\pi_{ij}$ which are not very environmentally similar. After all, our quest is to estimate growth between days where we in theory expect a similar amount of users through the marginalization of environmental factors. So, there comes a question, which pairs do we discount as days too disparate to be considered environmentally equivalent? Well, our good foresight in normalizing the continuous variables and weighing them properly (to the best of our abilities) means that we can immediately consult distribution of $l_{ij}$, which we see below:

```{r}
plot_loss_dist <- 
  df_loss %>%
    ggplot() +
    geom_histogram(aes(x = loss_ij), bins = 75, fill = "violetred3") +
    geom_vline(xintercept = mean(df_loss$loss_ij), color = "firebrick4") +
    labs(y = "Frequency", title = "Distribution of Loss for All Unique Ordered Pairs")
plot_loss_dist
```

A quick note: it should not be surprising that we obtain a distribution that resembles a $\chi^2$ distribution. After all, since we assume our predictors are (roughly) normal, their differences will be too; since our loss is essentially the norm of a multivariate normal, this explains our distribution shape! In any case, since we have done a good job by normalizing our variables (making our loss unit-less), a valid strategy is to \textbf{discount all pairs above a pre-specified upper bound}, which we will call $B$. An appopriate cutoff would probably be anywhere below the mean of our distribution $\mu \approx 6.86$, shown as the red line above. For example, we can take $B = \mu - 1\sigma \approx 3.35$ to be a reasonable cutoff for our upper loss bound $B$.

So, to summarize, we want to vet our exhaustive list of all possible day pairs $\Pi$ and throw out bad pairs after doing some "quality control" to obtain a filtered set of pairs $\tilde{\Pi}$. Let us take a brief moment to recap everything. We are interested in estimating $\g$, and one way we are interested in doing so is observing the growth in pairs of days where the environmental effects are marginalized. A strategy for doing so is to consider "reasonably good" pairs of days, which we explicitly quantify through filtering $\Pi$ through an upper bound on the loss $B$, which we may holistically choose based on the reasoning above. 

So, we obtain our filtered/selected set of day pairs $\tilde{\Pi}$, by cutting off the maximum loss at the selected upper bound $B$. That way, we can write our selected pair set as the output of a function $f_{loss}$ that \textit{filters} $\Pi$ with respect to the chosen loss upper bound $B$:
$$f_{loss}(\Pi, B) =\tilde{\Pi}_B = \{\pi_{ij} \mid l_{ij} = \mathcal{L}(\pi_{ij}) \leq B\}$$
Next, we also filter for what we call an "index difference bound". This is our second round of "quality control". Namely, the idea is simple: we don't want to pick days too close to each other. We know days close to each other will have similar environmental conditions, but as to avoid pesky autocorrelation and vaccuous/misleading information, we impose a bound on how close the days can be. 

Let $\rho := \min(i - j)$. In our lower-triangle matrix analogy (where we imagine indices as positions of matrix entries), $\rho$ denotes which diagonal band we are considering. In any case, by imposing a lower bound on $i - j$, to pass the index difference filter, every pair must be atleast $\rho$ days apart, or in other words, be at least $\rho$ diagonal bands away from the corner or towards the main diagonal. In practice, we find low values of $\rho$ to produce the most impactful filters, this makes sense, owing to the autocorrelation observed in days close to each other mentioned above. This makes the index difference bound much less critical than the loss bound, as it is much more impactful for low values of $\rho$. So, to summarize: 
$$f_{idx\_diff}(\Pi, \rho) =\tilde{\Pi}_\rho = \{\pi_{ij} \mid i - j \geq \rho\}$$
Finally... we can compute the estimate derived from the filtered pair set $\tilde{\Pi}$ as follows. Let $c_i$ denote the count of bike users in day $i$, and $c_j$ those in day $j$. \textbf{Since we have, in theory, marginalized out the environmental factors (and holidays), we may obtain a reasonable estimate for g by taking the observed growth observed between day $i$ and day $j$, then taking the average over all the pairs in our filtered pair set.} Since by construction, we used lower-triangular indices, this means day $j$ is always after day $i$. As such, we can imagine each pair, which has marginalized environmental factors, to contain true information about user-base growth since day $j$ is "environmentally equivalent" to day $i$ and is \textbf{in the future} of day $i$! As such, we could in theory attribute this growth to none other than the user base growth itself. To be more explicit, consider a filtered set of pairs $\tilde{\Pi}_{B,\rho}$. Since every pair is considered "good", from each pair $\pi_{ij}$ we obtain a preliminary estimate which we denote ${\gamma}_{ij} := \frac{c_j}{c_i}$. In general, the $\gamma_{ij}$ are slightly unstable, and their variance may be worth studying; this remains outside the scope of the paper. However, since we are aggregating pairs from a set of $n =  62,500$ pairs (in practice we choose less), there is an abundance of data to counterbalance this. So, to summarize, given a filtered set of pairs $\tilde{\Pi}_{B,\rho}$, we obtain $\hat{g}$ as by taking average of the set of estimates $\gamma_{ij}$ from every $\pi_{ij} \in \tilde{\Pi}_{B,\rho}$:
$$\hat{g} := \mathbb{E}[{{\gamma}_{ij}}] \text{ where } {\gamma}_{ij} = \frac{c_j}{c_i} \text{ and } \pi_{ij} \in \tilde{\Pi}_{B,\rho}$$
\blocktitle{Estimator Analysis in Bound Paramaters Space}
That being said, with our methodology explained, how does one estimate $\g$? This is our goal in the end! Well, as suggested by the notation above, the parameter estimate $\g$ is dependent on our filtering thresholds $B$ and $\rho$. Of course, any choice of $B_0$ and $\rho_0$ would be arbitrary, so our next course of action is to observe and analyze our estimator over the bound parameters space. To be concrete, we say fix some ranges $[B_0, B_1]$ and $[\rho_0, \rho_1]$, then observe the behaviour of the resulting parameter estimate $\g$. To do so, we use the `df_loss` previously mentioned in conjuction with the `get_df_param` function which takes these ranges as an input and outputs `df_param`. A glimpse into what our table looks like is shown below. Note that $n$ counts the number of pairs that "survive" the filtrations given the thresholds. In principle, we want our thresholds to pick exclusive pairs of days which are \textit{unusually} environmentally similar (low $B$) but far apart enough (high $\rho$), with emphasis on the former. That being said, we now plot the results from this dataframe and obtain the exciting results below!

```{r}
# Set a sequence of index bounds
idx_bds <- seq(122, 365, 8)
# Set a sequence of loss bounds: more critical
loss_bds <- seq(1, 4, 0.10)
# Obtain the parameter dataframe of g estimates
df_param <- df_loss %>% get_df_param(idx_bds, loss_bds)
# Peek at df_param
head(df_param)
```

```{r}
# Plot the estimates over bound parameter space
df_param %>% g_plot()
```

Again, as mentioned previously, the index difference bound is more sensitive when it comes to lower values. Greener points suggest that the index difference bound is high, so our days are quite far apart. Before we delve into the behaviour of the loss upper bound $B$, note that the opacity corresponds to the "percentage of pairs omitted". Recall that we want an exclusive set of pairs in which few pairs survive our strict filtrations or quality control. This is what the plot is suggesting, the more "faded" estimates of $\g$ are less well-founded, in fact completely unfounded as $p \to 1$ since we would indiscriminately be considering all pairs! As such, we conclude that the optimal range for $B$ probably lies between $[1,2]$, as these points have more "clarity" in their predictions due to their exclusively low loss. Otherwise, we also notice an interesting pattern with the index difference bound which makes sense, allowing days closer to each other will suggest "less growth". That being said, this means that in some sense, a potential optimal range for $\g$ could be that around the values where its variance starts to stabilize with respect to $\rho$. Visually, this is the range where the "length" of the streaks starts to hold still. For values before the critical $B \in [1,2]$ range, this corresponds to the values around $\g \in [1.4, 1.6]$. That being said, this analysis is holistic, and not $100\%$ conclusive. However, seeing this continous picture of estimating $g$ and narrowing down its true value to be close to $\g \in [1.4, 1.6]$ is not bad!

## Method II: The Window Technique

Jeez, that stuff was too technical. Thankfully, the window technique is much simpler. Remember our goal is to estiamte $\g$, which is the growth rate from 2011 to 2012. As such, the most elementary method to use based on this principle is to take the ratio of the counts in the first day of 2011 and those in the last day. This is justified because everyone would agree, there is some notion that Dec 31, 2011 $\approx$ Jan 1, 2012. However, the issue with this is that our estimate is based on one pair of points $(n = 1)$. 

As such, to preserve the spirit of the principle, but generalize as to instill more certainty and stability in our estiamte, we describe the "window technique". Firstly, we will say that the method mentioned above is the window technique, for a window size of $w = 1$. The principle of the window technique is the same as for the method described above, the days at the end of the year of 2011 will be similar to those in the beginning of 2012. So, the best we can do is take an equivalent "window" or sample of days from both the beginning and end of the year of size $w$. This way, we use more data and hence have more stability. Then, using those selected $w$ days, we compute the average `cnt` in the beginning (first $w$ days) and end (last $w$ days), then take the ratio of those averages to estimate $\g$. We call this estimate $\g_w$. In our first example, we were describing $\g_1$. 

In any case, just like before, $\g$ is an estimator based on a parameter, in this case $w$. We must note that as $w$ increases, our assmuptions about "day similarity" between the end of 2011 and start of 2012 slowly starts to weaken. As such, we cannot go crazy and choose large $w$. Below, we exhaustively compute $\g_w$ for $w = 1,2,\dots,20$, and plot our estimates with respect to $w$.
```{r}
# Obtain the table of g esimates by window paramater w
tbl_window <- purrr::map_dfr(1:20, function(w){data.frame(w = w, g_w = window_g(w))})
# Plot the values
plot_w <- plot_window(tbl_window)
plot_w
```
There are a few things to consider. As expected, the values $w \in [1,4]$ exhibit highly unstable behaviour in estimating $\g$. However, it starts to stabilize around $w = 6$, highlighted in red. As it turns out, $w = 6$, holistically speaking is our best $w$ for a few reasons. Firstly, going beyond $w = 6$ would mean including Christmas, which is a holiday and would thus throw off our estiamtes. Furthermore, the values of $\g$ start to stablize reasonably well around that value, indicating the marginal benefit of increasing $w$ is almost fully realized at this point; remember, our assumptions about day similarity break quickly as $w$ grows. Note that in the plot below, we compute $\g$ after omitting Jan 3rd, due to its high environmental loss value when paired with the other values. Lastly, one reason we consider $w = 6$ to be a good candidate is the fact that the estimate $\g$ starts to steadily increase again, indicating suspicious behaviour which could be attributed to the fact that as the window increases, we include days whose temperatures are changing in potentially different directions (generally both warming). 

That being said, there is one curious piece of information corroborated by the environmental loss function technique. Namely, both graphs seem to suggest discounting the values $\g \gg 1.6$. In the second graph, this is suggested by the vacously increasing value of $\g$ with respect to $w$ starting around $w = 10$, indicating that the point in which there is "good" information about $\g$ probably lies before that value of $w = 10$, corroborated also by the simple principle that smaller $w$ is generally better in terms of upholding assumptions. So, with all this information considered, we settle for $\g := \g_6 \approx 1.613$, which is the most compatible value from our holistic analysis of both of our estimation methods.

# Discussion 

```{r, fig.height = 5, fig.cap = "Various g and prediction adjustment"}
fitted_vals <- predict(mod.cas.2.final, data2012) + predict(mod.reg.2.final, data2012)
LB <- 1.4; UB <- 1.613; alpha0 <- 0.4
plot_gfac2 <- plot_user_counts(data2012, user = "cnt") + 
  geom_line(data2012, mapping = aes(dteday, fitted_vals), color = "steelblue3", size = 0.5, alpha = 0.6) +
  geom_line(data2012, mapping = aes(dteday, fitted_vals*UB), color = "limegreen", size = 0.5, alpha = alpha0) +
  geom_line(data2012, mapping = aes(dteday, fitted_vals*LB), color = "red", size = 0.5, alpha = alpha0) +
  labs(title = "Daily Count Predictions: Adjusted for Various g Esimates")
plot_gfac2
```



Models for both long-term and short-term predictions are included. 

To be noticed, at least one more year's data is needed for a final validation of the refined model, which is not available for the moment. This is to be left for the future work.  

Time series 

# References

[1] Fanaee-T, Hadi, and Gama, Joao, "Event labeling combining ensemble detectors and background knowledge", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.

# Appendix

## Data

- `instant`: record index
- `dteday`: date
- `season`: season (1:spring, 2:summer, 3:fall, 4:winter)
- `yr`: year (0:2011, 1:2012)
- `mnth`: month ( 1 to 12)
- `hr`: hour (0 to 23)
- `holiday`: weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
- `weekday`: day of the week
- `workingday`: if day is neither weekend nor holiday is 1, otherwise is 0.
+ `weather-sit`: 
	- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
	- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
	- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
	- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- `temp`: Normalized temperature in Celsius. The values are divided to 41 (max)
- `atemp`: Normalized feeling temperature in Celsius. The values are divided to 50 (max)
- `hum`: Normalized humidity. The values are divided to 100 (max)
- `windspeed`: Normalized wind speed. The values are divided to 67 (max)
- `casual`: count of casual users
- `registered`: count of registered users
- `cnt`: count of total rental bikes including both casual and registered

## Preprocessing

### Type Conversion
(codes here)

### Value Conversion
(codes here)

## Variable Selection 

### Predictors Selection
(Figure ?: Scatter plot matrix for Group 1 predictors)

(Figure ?: Scatter plot matrix for Group 2 predictors)

### Response Transformation

## Initial Modeling

### Beginning Model 

### Final Model

## Diagonostic Analysis 

(Figure ?: 2011 fitted vs residual plot and normal qq-plot for registered counts)

(Figure ?: 2011 fitted vs residual plot and normal qq-plot for casual counts)

## Validation and Problemshooting

## Refined Model 

### Prediction of the Yearly Growth Ratio

### Prediction without the Yearly Growth Ratio

